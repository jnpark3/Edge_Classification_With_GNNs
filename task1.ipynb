{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset  # not the one from PyG!\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch.nn.functional import relu, binary_cross_entropy_with_logits, linear\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDataset class for handling file loading\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path: Path):\n",
    "        super().__init__()\n",
    "        self.graphs = list(path.glob(\"*.pt\"))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.graphs[idx])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in set: 1000\n",
      "Set of keys in each graph ['y', 'edge_attr', 'x', 'edge_index']\n",
      "Number of nodes in first graph: 419\n",
      "Number of edges in first graph: 4882\n",
      "Number of node features: 6\n",
      "Number of edge features: 4\n",
      "Is this graph undirected?  False\n",
      "Number of edge features: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Load first batch (batch_1_0)\n",
    "\n",
    "dataset = MyDataset(Path(\"./dataset/batch_1_0\"))\n",
    "\n",
    "# Preliminary analysis of dataset behavior\n",
    "\n",
    "print(\"Number of graphs in set:\", len(dataset))\n",
    "print(\"Set of keys in each graph\", dataset[0].keys)\n",
    "print(\"Number of nodes in first graph:\", dataset[0].num_nodes)\n",
    "print(\"Number of edges in first graph:\", dataset[0].num_edges)\n",
    "print(\"Number of node features:\", dataset[0].num_node_features)\n",
    "print(\"Number of edge features:\", dataset[0].num_edge_features)\n",
    "print(\"Is this graph undirected? \", dataset[0].is_undirected())\n",
    "\n",
    "print(\"Number of edge features:\", dataset[0]['y'])\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, sample, hidden_layers = [8], intra_hidden = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = []\n",
    "        self.linear_weights = torch.randn(2 * sample.num_node_features + sample.num_edge_features)\n",
    "\n",
    "        if len(hidden_layers) == 0:\n",
    "            # Case when no hidden layers are passed\n",
    "\n",
    "            nn = torch.nn.Sequential(\n",
    "                torch.nn.Linear(sample.num_edge_features, intra_hidden),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(intra_hidden, sample.num_node_features)\n",
    "            )\n",
    "            self.conv.append(NNConv(sample.num_node_features, 1, nn))\n",
    "\n",
    "        else:\n",
    "            # Case when hidden layers are passed\n",
    "\n",
    "            nn = self.simple_nn(sample.num_edge_features, intra_hidden, sample.num_node_features * hidden_layers[0])\n",
    "            self.conv.append(NNConv(sample.num_node_features, hidden_layers[0], nn))\n",
    "\n",
    "            for i in range(len(hidden_layers) - 1):\n",
    "                nn = self.simple_nn(sample.num_edge_features, intra_hidden, hidden_layers[i] * hidden_layers[i + 1])\n",
    "                self.conv.append(NNConv(hidden_layers[i], hidden_layers[i + 1], nn))    \n",
    "            \n",
    "            nn = self.simple_nn(sample.num_edge_features, intra_hidden, intra_hidden * hidden_layers[-1])\n",
    "            self.conv.append(NNConv(hidden_layers[-1], intra_hidden, nn))\n",
    "\n",
    "        # Last layer for making edge predictions from node + edge data\n",
    "        self.edge_predictor = self.simple_nn(sample.num_edge_features + intra_hidden, intra_hidden, 1)\n",
    "\n",
    "        self.conv_module = torch.nn.ModuleList(self.conv)\n",
    "    \n",
    "    def simple_nn(self, a_1, a_2, a_3):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Linear(a_1, a_2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(a_2, a_3)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "        '''\n",
    "        # Concatenate the edge attributes with the source node attributes\n",
    "        '''\n",
    "    \n",
    "        for i in range(len(self.conv)):\n",
    "            x = relu(self.conv[i](x, edge_index, edge_attr))\n",
    "\n",
    "        src_node_attrs = x[edge_index[0]]\n",
    "        conv_attributes = torch.cat((edge_attr, src_node_attrs), dim=1)\n",
    "        edge_scores = self.edge_predictor(conv_attributes)\n",
    "\n",
    "        return edge_scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n",
      "1 of 2\n",
      "2 of 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = GNN(dataset[0]).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = binary_cross_entropy_with_logits(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649588312d4cf9672288d95fa805e14ed1c14f756baea9d0a14e0f27f774ca24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
